# Phase 1: Fundamentals (Mental Model First)

## 1. What Problem Does Kubernetes Actually Solve?

To understand Kubernetes (K8s), we first have to look at the evolution of how we deploy software.

### The Mental Model: The Infrastructure Tetris

Imagine you are a logistics manager for a shipping company.

1.  **The VM Era (The Big Trucks):**
    - In the old days, if you wanted to ship a package (your app), you had to hire a whole dedicated truck (a Virtual Machine).
    - **Problem:** Even if your package was small, you paid for the whole truck. It was heavy, slow to start, and if the truck broke down, your package was stuck until you manually moved it to another truck.

2.  **The Container Era (The Standard Boxes):**
    - Docker came along and said: "Let's put everything in standard-sized boxes (Containers)." These boxes are light and can fit in any truck.
    - **Improvement:** You can now pack many boxes into one truck. It's much more efficient.
    - **New Problem:** Who decides which box goes into which truck? If you have 1,000 boxes and 50 trucks, doing this by hand is a nightmare. This is **Bin Packing**.

3.  **The Kubernetes Era (The Automated Warehouse):**
    - Kubernetes is the **automated crane and logistics system** for your warehouse.
    - You don't tell the system: "Put Box A into Truck 1."
    - You tell the system: **"I want 3 copies of Box A to be somewhere in the warehouse at all times. I don't care where."**

    - **Definition:** Kubernetes l√† m·ªôt h·ªá th·ªëng ƒëi·ªÅu ph·ªëi (orchestrator) ƒë·ªÉ ch·∫°y v√† qu·∫£n l√Ω container ·ªü quy m√¥ l·ªõn.

### Why do we need this? (The Value Proposition)

- **High Availability:** If a worker node (truck) crashes, K8s notices immediately and moves the containers to a healthy node.
- **Scalability:** If traffic spikes, you tell K8s "Give me 10 more boxes," and it finds space for them.
- **Resource Efficiency:** K8s plays "Infrastructure Tetris." It looks at how much CPU/RAM each box needs and packs them tightly to save you money on cloud bills.

### What happens if we don‚Äôt use it?

If you try to run complex systems with just Docker:

- **Manual Resuscitation:** When an app crashes at 3 AM, _you_ are the one who has to SSH in and restart the container.
- **Zombies:** You'll have containers running on servers that nobody remembers, eating up resources.
- **The Spreadsheet of Doom:** You'll end up keeping a manual list of which app is on which IP address, which will eventually go out of date, leading to a massive outage.

### What breaks in production?

In the early stages, people often fail to give Kubernetes enough "information" to do its job.
If you don't define **Resource Limits** (telling K8s how big the box is), one greedy box can grow until it takes over the whole truck, causing all other boxes in that truck to "suffocate" and crash. This is the **"Noisy Neighbor"** problem.

---

### Key Takeaways

- **Kubernetes is NOT a container engine.** It is a container **orchestrator**. It manages the lifecycle of containers.
- The primary goal is to **automate the operational tasks** that humans used to do manually.
- It shifts the focus from "Machine Management" to "Application Management."

### Common Mistakes

- Thinking Kubernetes "makes things simpler." It doesn't. It makes **complexity manageable**.
- Using K8s for a single static website. That's like hiring a logistics fleet to deliver a single letter. Use a simpler tool (Vercel, Cloud Run) instead.

### Production Tips

- Always think in terms of **Desired State**, not commands. Don't think "restart pod," think "I want this many pods running."

---

### üí° Knowledge Check

If you have a cluster of 5 servers and you need to ensure your "Payment Service" is always running 3 instances for redundancy, what is the fundamental difference between doing this manually (e.g., with SSH + Docker) versus letting Kubernetes handle it?

---

## 2. Control Plane vs. Worker Node: The Brain and the Muscle

In Kubernetes, we divide the world into two main roles. It‚Äôs a classic **Manager-Worker** relationship.

### The Mental Model: The Restaurant

- **The Control Plane (The Front of House / Manager):**
  - This is where the manager sits. They take orders, keep the menu (config), look at the floor map, and decide which waiter is available to handle a specific table.
  - The manager doesn't cook the food; they **coordinate**.

  - **Definition:** Control Plane l√† ‚Äúb·ªô n√£o‚Äù c·ªßa Kubernetes cluster.
    N√≥ ch·ªãu tr√°ch nhi·ªám:
    - nh·∫≠n l·ªánh t·ª´ b·∫°n (kubectl, CI/CD)
    - l∆∞u tr·∫°ng th√°i cluster
    - quy·∫øt ƒë·ªãnh Pod ch·∫°y ·ªü ƒë√¢u
    - ƒë·∫£m b·∫£o cluster lu√¥n ti·∫øn v·ªÅ ‚Äúdesired state‚Äù
      üëâ Worker Nodes ch·∫°y workload.
      üëâ Control Plane ƒëi·ªÅu ph·ªëi v√† qu·∫£n l√Ω m·ªçi th·ª©.

- **The Worker Nodes (The Kitchen / Cooks):**
  - These are the actual servers (VMs or physical machines) where your containers run.
  - They are the muscle. They do the heavy lifting: running the containers, handling the networking of the app, and reporting back to the manager: "I'm busy" or "I have space for more."

  - Node l√† m·ªôt m√°y (VM ho·∫∑c physical server) trong Kubernetes cluster.
  - Worker Node l√† Node c√≥ nhi·ªám v·ª• ch·∫°y workload (Pods/Containers).
  - H√£y t∆∞·ªüng t∆∞·ª£ng cluster nh∆∞ m·ªôt nh√† m√°y:
    - Control Plane = qu·∫£n l√Ω + ƒëi·ªÅu ph·ªëi
    - Worker Nodes = c√¥ng nh√¢n + m√°y m√≥c s·∫£n xu·∫•t
    - Pods = c√°c c√¥ng vi·ªác c·ª• th·ªÉ

### ASCII Architecture

```text
       +---------------------------------------+
       |            CONTROL PLANE              |
       |  (The Brain - Decisions Happen Here)   |
       |                                       |
       |   +-----------+       +-----------+   |
       |   | API Server| <---> |    etcd   |   |
       |   +-----------+       +-----------+   |
       |         ^                   |         |
       |         |         +-----------------+ |
       |         v         | Scheduler       | |
       |   +-----------+   +-----------------+ |
       |   | Controller|                       |
       |   | Manager   |                       |
       |   +-----------+                       |
       +---------------------------------------+
                        |
            (Commands / Manifests via API)
                        |
       +---------------------------------------+
       |             WORKER NODES              |
       |         (The Muscle - Apps Run Here)  |
       |                                       |
       |  +----------+         +----------+    |
       |  | Node 1   |         | Node 2   |    |
       |  | [Pod A]  |         | [Pod B]  |    |
       |  | [Pod C]  |         |          |    |
       |  +----------+         +----------+    |
       +---------------------------------------+
```

### Why do we need this separation?

1.  **Fault Tolerance:** If a Worker Node dies, the Control Plane is still alive to notice it and reschedule the work.
2.  **Scalability:** You can add 1,000 more Worker Nodes without changing how you interact with the Control Plane.
3.  **Security:** You can restrict access so developers only talk to the API Server (Control Plane) and never have SSH access to the Worker Nodes themselves.

### What happens if we don‚Äôt use it? (The "God Node" Problem)

In old-school setups, we often ran the app and the configuration tools on the same machine.

- If that machine's kernel panicked, you lost both your app **and** your ability to fix it.
- Separating them means the "Governor" is isolated from the "Factory Floor."

### What breaks in production?

The biggest production nightmare is **"Brain Surgery" issues on the Control Plane**:

1.  **etcd instability:** If the database of the Control Plane (etcd) is slow or has disk latency, the entire cluster becomes "frozen." You can't start new pods or delete old ones. The cluster becomes a "read-only" graveyard.
2.  **API Server Overload:** If you have too many automated scripts hitting the API at once, the Control Plane can't respond to the Worker Nodes, leading to a "Split Brain" where nodes think they are alone and start doing weird things.

---

### Key Takeaways

- **Control Plane = Brain.** It manages the state and makes decisions.
- **Worker Node = Muscle.** It executes the work (containers).
- Production clusters usually have **multiple Control Plane nodes** (High Availability) so that if one manager goes on break, the others keep the restaurant running.

### Common Mistakes

- Running heavy workloads on the Control Plane nodes. Never do this. Keep them dedicated to management.
- Thinking that if the Control Plane goes down, your apps stop. **False.** Your apps (on Worker Nodes) keep running, but you lose the ability to change anything (no scaling, no updates).

### Production Tips

- Treat your Worker Nodes as **"Cattle, not Pets."** You should be able to kill a Worker Node at any time and trust the Control Plane to fix the situation.
- Always monitor **etcd latency**. It is the heartbeat of your cluster.

---

### üí° Knowledge Check

If the Control Plane's API Server becomes unreachable for 1 hour, what happens to the users currently using your application? Can you deploy a new version of the app during this hour?

---

## 3. Pod, Container, Image: Why the "Pod" exists?

You already know what a **Container** is (your app in a box) and what an **Image** is (the blueprint for that box). But why did Kubernetes invent the **Pod**?

### The Mental Model: The Space Capsule

Imagine a mission to Mars.

- The **Image** is the blueprint for the capsule.
- The **Container** is the capsule actually sitting on the launchpad.
- The **Pod** is the **Life Support System** wrapped around that container.

‚úÖ **Pod l√† ƒë∆°n v·ªã nh·ªè nh·∫•t m√† Kubernetes deploy v√† qu·∫£n l√Ω.**

Pod l√† m·ªôt ‚Äúwrapper‚Äù ch·∫°y **1 ho·∫∑c nhi·ªÅu containers** c√πng nhau, chia s·∫ª:

- Network (IP + port space)
- Storage volumes
- Lifecycle (start/stop together)

üëâ Kubernetes kh√¥ng deploy container tr·ª±c ti·∫øp.
N√≥ deploy Pod.

In a pod, you might have:

1.  **The Main App:** The astronaut (your code).
2.  **The Sidecar:** A small robot that handles oxygen (logging/proxy/monitoring).

### Why do we need the Pod?

Containers are designed to run **exactly one process**. But in the real world, apps need "utility" processes to run right next to them.
A Pod allows two containers to:

1.  **Share the same IP:** They can talk to each other via `localhost`.
2.  **Share the same Filesystem:** They can both read/write to the same folder.
3.  **Die Together:** If the Pod is moved to another server, both containers move together.

### What happens if we don‚Äôt use it?

If K8s only managed individual containers, you'd have to manually coordinate that "App A" and its "Logger B" always end up on the same physical server. If they ended up on different servers, your logger wouldn't be able to see the app's files. It would be an orchestration nightmare.

### What breaks in production?

The **"Multiple Astronauts"** mistake.
Beginners often try to put two _completely different_ apps (like a Web App and a Database) in the same Pod because it "feels easier."
**Don't do this.** If the web app needs to scale to 10 copies but the DB only needs 1, you can't split them. Only put things in the same Pod if they **must** share a lifecycle.

---

## 4. The Magic Under the Hood: Internal Components

Let's look at the "Org Chart" of the Control Plane.

### 1. kube-apiserver (The Gatekeeper)

- **What it is:** The only way to talk to the cluster. Every command (kubectl, UI, even other K8s components) goes through here.
- **Production Tip:** If this is slow, nothing happens. It's the front door.

**kube-apiserver** l√† th√†nh ph·∫ßn **quan tr·ªçng nh·∫•t** v√† l√† **tr√°i tim** c·ªßa **Kubernetes control plane** (m·∫∑t ph·∫≥ng ƒëi·ªÅu khi·ªÉn).

N√≥i ƒë∆°n gi·∫£n:

kube-apiserver ch√≠nh l√† **"c·ªïng v√†o duy nh·∫•t"** ch√≠nh th·ª©c ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi c·ª•m Kubernetes.

N√≥ cung c·∫•p **Kubernetes API** (d∆∞·ªõi d·∫°ng HTTP REST API) ƒë·ªÉ m·ªçi ng∆∞·ªùi v√† m·ªçi th√†nh ph·∫ßn kh√°c giao ti·∫øp v·ªõi c·ª•m.

### Vai tr√≤ ch√≠nh c·ªßa kube-apiserver

- L√† **frontend** c·ªßa to√†n b·ªô control plane
- **Ti·∫øp nh·∫≠n** t·∫•t c·∫£ c√°c y√™u c·∫ßu (t·ª´ ng∆∞·ªùi d√πng l·∫´n t·ª´ c√°c th√†nh ph·∫ßn h·ªá th·ªëng)
- **X√°c th·ª±c** (authentication) ‚Üí ki·ªÉm tra xem b·∫°n c√≥ quy·ªÅn kh√¥ng
- **Ph√¢n quy·ªÅn** (authorization) ‚Üí ki·ªÉm tra b·∫°n ƒë∆∞·ª£c l√†m g√¨
- **Ki·ªÉm tra h·ª£p l·ªá** (validation) c·ªßa y√™u c·∫ßu
- **L∆∞u tr·ªØ** tr·∫°ng th√°i mong mu·ªën v√†o **etcd** (sau khi ƒë√£ validate)
- **Tr·∫£ v·ªÅ** tr·∫°ng th√°i hi·ªán t·∫°i khi ƒë∆∞·ª£c h·ªèi (get, list, watch...)

### Ai giao ti·∫øp v·ªõi kube-apiserver?

| Ai g·ªçi                    | V√≠ d·ª• l·ªánh / h√†nh ƒë·ªông                           | M·ª•c ƒë√≠ch                     |
| ------------------------- | ------------------------------------------------ | ---------------------------- |
| Ng∆∞·ªùi d√πng                | `kubectl get pods`, `kubectl apply -f ...`       | Qu·∫£n l√Ω t√†i nguy√™n           |
| kube-controller-manager   | Theo d√µi v√† ƒëi·ªÅu ch·ªânh Deployment, ReplicaSet... | Duy tr√¨ tr·∫°ng th√°i mong mu·ªën |
| kube-scheduler            | Xem pod n√†o ch∆∞a ƒë∆∞·ª£c g√°n node ‚Üí ch·ªçn node       | Quy·∫øt ƒë·ªãnh ƒë·∫∑t pod ·ªü ƒë√¢u     |
| kubelet (tr√™n worker)     | B√°o c√°o tr·∫°ng th√°i pod, nh·∫≠n l·ªánh ch·∫°y pod m·ªõi   | Qu·∫£n l√Ω container th·ª±c t·∫ø    |
| kube-proxy                | Theo d√µi Service/Endpoint thay ƒë·ªïi               | C·∫≠p nh·∫≠t iptables / IPVS     |
| CI/CD pipeline            | Helm install, ArgoCD sync, GitOps...             | Tri·ªÉn khai ·ª©ng d·ª•ng t·ª± ƒë·ªông  |
| Horizontal Pod Autoscaler | Theo d√µi metrics ‚Üí t·∫°o/x√≥a pod                   | T·ª± ƒë·ªông scale                |

T√≥m l·∫°i: **H·∫ßu nh∆∞ kh√¥ng c√≥ h√†nh ƒë·ªông n√†o trong Kubernetes c√≥ th·ªÉ x·∫£y ra m√† kh√¥ng ƒëi qua kube-apiserver**.

### 2. etcd (The Source of Truth)

- etcd l√† m·ªôt d·ª± √°n m√£ ngu·ªìn m·ªü (thu·ªôc CNCF, ƒë√£ graduated).
- L√† **distributed key-value store** ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ:
  - **Nh·∫•t qu√°n m·∫°nh** (strong consistency) nh·ªù thu·∫≠t to√°n **Raft**.
  - **T√≠nh s·∫µn s√†ng cao** (highly available) khi ch·∫°y theo c·ª•m (th∆∞·ªùng 3 ho·∫∑c 5 node).
  - **ƒê·ªô tin c·∫≠y cao** cho d·ªØ li·ªáu quan tr·ªçng trong h·ªá th·ªëng ph√¢n t√°n.

Kubernetes ch·ªçn etcd l√†m **backing store** ch√≠nh th·ª©c v√¨ n√≥ ƒë√°p ·ª©ng ho√†n h·∫£o nhu c·∫ßu c·ªßa control plane.

### 3. kube-scheduler (The Matchmaker)

**kube-scheduler** l√† th√†nh ph·∫ßn **b·ªô l·∫≠p l·ªãch** (scheduler) m·∫∑c ƒë·ªãnh trong **Kubernetes control plane**, ch·ªãu tr√°ch nhi·ªám **quy·∫øt ƒë·ªãnh Pod s·∫Ω ch·∫°y tr√™n Node n√†o** trong c·ª•m.

N√≥i ng·∫Øn g·ªçn:  
Khi b·∫°n t·∫°o m·ªôt Pod (ho·∫∑c Deployment/StatefulSet/... t·∫°o ra Pod), Pod ban ƒë·∫ßu ·ªü tr·∫°ng th√°i **Pending** (ch∆∞a ƒë∆∞·ª£c g√°n Node). kube-scheduler s·∫Ω "nh√¨n" to√†n b·ªô c·ª•m, t√¨m Node ph√π h·ª£p nh·∫•t d·ª±a tr√™n t√†i nguy√™n, r√†ng bu·ªôc, ∆∞u ti√™n... r·ªìi **g√°n** Pod v√†o Node ƒë√≥ b·∫±ng c√°ch c·∫≠p nh·∫≠t tr∆∞·ªùng `nodeName` trong spec c·ªßa Pod.

Sau khi ƒë∆∞·ª£c g√°n, **kubelet** tr√™n Node ƒë√≥ m·ªõi nh·∫≠n Pod v√† kh·ªüi ch·∫°y container th·ª±c t·∫ø.

### Vai tr√≤ ch√≠nh c·ªßa kube-scheduler

| Vai tr√≤                     | Gi·∫£i th√≠ch chi ti·∫øt                                                                                                        |
| --------------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| **Watch Pod ch∆∞a g√°n Node** | Theo d√µi qua kube-apiserver c√°c Pod m·ªõi t·∫°o ho·∫∑c Pending (kh√¥ng c√≥ nodeName)                                               |
| **L·ªçc (Filtering)**         | Lo·∫°i b·ªè c√°c Node kh√¥ng ƒë√°p ·ª©ng y√™u c·∫ßu (thi·∫øu CPU/RAM, kh√¥ng match nodeSelector, taints kh√¥ng tolerate, affinity rules...) |
| **Ch·∫•m ƒëi·ªÉm (Scoring)**     | ƒê√°nh gi√° c√°c Node c√≤n l·∫°i theo ƒëi·ªÉm s·ªë (v√≠ d·ª•: Node c√≥ nhi·ªÅu t√†i nguy√™n tr·ªëng h∆°n ‚Üí ƒëi·ªÉm cao h∆°n)                          |
| **Ch·ªçn Node t·ªët nh·∫•t**      | Bind Pod v√†o Node c√≥ ƒëi·ªÉm cao nh·∫•t (ho·∫∑c random n·∫øu b·∫±ng nhau)                                                             |
| **C·∫≠p nh·∫≠t tr·∫°ng th√°i**     | Ghi `nodeName` v√†o Pod object qua apiserver ‚Üí etcd l∆∞u tr·ªØ ‚Üí kubelet nh·∫≠n v√† ch·∫°y                                          |

‚Üí kube-scheduler **kh√¥ng kh·ªüi ch·∫°y container**, ch·ªâ quy·∫øt ƒë·ªãnh "ƒë·∫∑t ·ªü ƒë√¢u". N√≥ gi·ªëng nh∆∞ m·ªôt "nh√¢n vi√™n ph√¢n c√¥ng vi·ªác" th√¥ng minh.

### 4. kube-controller-manager (The Repairman)

**kube-controller-manager** l√† th√†nh ph·∫ßn **"ng∆∞·ªùi gi√°m s√°t v√† t·ª± ƒë·ªông s·ª≠a ch·ªØa"** (reconciler) trong **Kubernetes control plane**, ch·ªãu tr√°ch nhi·ªám ch·∫°y h√†ng lo·∫°t **controller** (c√°c v√≤ng l·∫∑p ƒëi·ªÅu khi·ªÉn) ƒë·ªÉ ƒë·∫£m b·∫£o **tr·∫°ng th√°i th·ª±c t·∫ø (actual state)** c·ªßa c·ª•m lu√¥n kh·ªõp v·ªõi **tr·∫°ng th√°i mong mu·ªën (desired state)** m√† b·∫°n khai b√°o.

- L√† m·ªôt **daemon** (qu√° tr√¨nh ch·∫°y li√™n t·ª•c) nh√∫ng nhi·ªÅu **core controllers** m·∫∑c ƒë·ªãnh c·ªßa Kubernetes v√†o m·ªôt binary duy nh·∫•t.
- M·ªói controller l√† m·ªôt **v√≤ng l·∫∑p ƒëi·ªÅu khi·ªÉn** (control loop) ki·ªÉu:  
  **Watch ‚Üí Compare ‚Üí Reconcile** (xem tr·∫°ng th√°i ‚Üí so s√°nh v·ªõi desired ‚Üí h√†nh ƒë·ªông ƒë·ªÉ s·ª≠a n·∫øu l·ªách).
- T·∫•t c·∫£ controller ƒë·ªÅu giao ti·∫øp **qua kube-apiserver** (watch/list/update/delete c√°c object).

| Controller                          | Vai tr√≤ ch√≠nh                                                      | V√≠ d·ª• th·ª±c t·∫ø                                        |
| ----------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------- |
| **ReplicationController**           | Duy tr√¨ s·ªë l∆∞·ª£ng pod ch√≠nh x√°c theo replicas (c≈©, √≠t d√πng)         | Gi·ªØ ƒë√∫ng 3 pod cho RC c≈©                             |
| **ReplicaSet**                      | T∆∞∆°ng t·ª±, nh∆∞ng d√πng cho Deployment                                | Deployment scale ‚Üí ReplicaSet t·∫°o/x√≥a pod            |
| **Deployment**                      | Qu·∫£n l√Ω rollout, rollback, scaling Deployment                      | Rolling update, pause/resume, history                |
| **StatefulSet**                     | Qu·∫£n l√Ω pod c√≥ tr·∫°ng th√°i (ordered, unique identity)               | T·∫°o pod theo th·ª© t·ª±, gi·ªØ PVC g·∫Øn li·ªÅn                |
| **DaemonSet**                       | ƒê·∫£m b·∫£o 1 pod ch·∫°y tr√™n m·ªçi (ho·∫∑c m·ªôt s·ªë) node                     | node-exporter, logging agent ch·∫°y tr√™n t·∫•t c·∫£ node   |
| **Job / CronJob**                   | Qu·∫£n l√Ω task ch·∫°y m·ªôt l·∫ßn ho·∫∑c ƒë·ªãnh k·ª≥                             | Batch job ho√†n th√†nh ‚Üí x√≥a pod                       |
| **Node Controller**                 | Theo d√µi node status, ƒë√°nh d·∫•u NotReady, x√≥a pod khi node ch·∫øt l√¢u | Node down 5 ph√∫t ‚Üí evict pod ƒë·ªÉ scheduler reschedule |
| **Endpoints Controller**            | T·∫°o v√† c·∫≠p nh·∫≠t Endpoints cho Service                              | Pod ready ‚Üí th√™m IP:port v√†o Endpoints               |
| **Service Account Controller**      | T·∫°o ServiceAccount m·∫∑c ƒë·ªãnh cho namespace                          | T·ª± ƒë·ªông t·∫°o default SA                               |
| **Namespace Controller**            | X√≥a t√†i nguy√™n khi namespace b·ªã x√≥a                                | X√≥a t·∫•t c·∫£ pod, service... trong namespace           |
| **Garbage Collector**               | X√≥a object con khi owner b·ªã x√≥a (ownerReferences)                  | X√≥a pod khi ReplicaSet b·ªã x√≥a                        |
| **TTL Controller**                  | X√≥a job sau th·ªùi gian s·ªëng (TTL)                                   | Job h·∫øt h·∫°n ‚Üí t·ª± x√≥a                                 |
| **Horizontal Pod Autoscaler (HPA)** | Theo d√µi metrics ‚Üí scale replicas l√™n/xu·ªëng                        | CPU > 80% ‚Üí tƒÉng replicas                            |

---

| Th√†nh ph·∫ßn                  | Vai tr√≤ ch√≠nh                                     | Ai truy c·∫≠p ch√≠nh?         | N·∫øu ch·∫øt th√¨ sao?                       |
| --------------------------- | ------------------------------------------------- | -------------------------- | --------------------------------------- |
| **kube-apiserver**          | C·ªïng v√†o, validate, auth, serve API               | kubectl, t·∫•t c·∫£ components | Kh√¥ng t·∫°o/xem ƒë∆∞·ª£c object m·ªõi           |
| **etcd**                    | L∆∞u tr·ªØ tr·∫°ng th√°i vƒ©nh vi·ªÖn                      | Ch·ªâ apiserver              | C·ª•m t√™ li·ªát, m·∫•t to√†n b·ªô tr·∫°ng th√°i     |
| **kube-scheduler**          | Quy·∫øt ƒë·ªãnh Pod ch·∫°y tr√™n Node n√†o                 | T·ª± ƒë·ªông (watch apiserver)  | Pod m·ªõi v·∫´n Pending m√£i, kh√¥ng t·ª± scale |
| **kube-controller-manager** | Duy tr√¨ tr·∫°ng th√°i (reconcile Deployment, HPA...) | T·ª± ƒë·ªông                    | Deployment kh√¥ng scale/recreate Pod     |

---

### Key Takeaways

- **Pod** = The smallest unit K8s manages. It's a wrapper for one or more containers.
- **API Server** is the gateway; **etcd** is the memory.
- **Scheduler** decides _where_; **Controller** ensures _how many_.

### Production Tips

- Always use **Sidecars** for cross-cutting concerns (logging, security) rather than baking them into your main app image. This keeps your app "clean."

---

### üí° Knowledge Check

If you want to run a "Log Relayer" that sends your app's logs to a central server, should you put it in the same Pod as your App, or a separate Pod? Why?

---

## 5. Declarative vs. Imperative: Why Intent Matters

This is the biggest "click" moment for new Kubernetes users.

- **Imperative (How):** "Go to the kitchen, pick up a pan, crack an egg, and fry it."
- **Declarative (What):** "I want a fried egg on my plate."

In traditional scripting (Imperative), you tell the server:

1.  `docker run my-app`
2.  `docker network connect my-net`
3.  `docker expose 80`

**In Kubernetes (Declarative), you write a Manifest (YAML):**
"I want 3 copies of this image, reachable on port 80."

### Why do we need this?

If you use Imperative commands and a server crashes, you have to run those commands again. If you use Declarative manifests, **Kubernetes knows your intent**. If a server crashes, K8s looks at your "desired state" (the YAML) and automatically restarts everything to match it.

### What happens if we don‚Äôt use it?

Without it, you have to write complex "Health Check" scripts that constantly ping your servers and run `docker restart` if they fail. You end up building a "Poor Man's Kubernetes" that is buggy and hard to maintain.

---

## 6. The Reconciliation Loop: The Heartbeat of K8s

How does K8s actually "decide" what to do? It's a simple, never-ending loop:

1.  **Observe:** Look at the cluster (Current State).
2.  **Diff:** Compare it to the YAML manifests (Desired State).
3.  **Act:** If they don't match, make changes to make them match.

### The Mental Model: The Thermostat

- **Desired State:** You set the thermometer to 72¬∞F.
- **Observation:** The room is 68¬∞F.
- **Action:** The thermostat turns on the heater.
- **Observation:** The room is 72¬∞F.
- **Action:** The thermostat does nothing (matches desired state).

### What breaks in production?

**"Fight for Reality."**
Sometimes, two different controllers might have conflicting "Desired States."

- Controller A says: "I want 3 pods."
- Controller B says: "I want 0 pods."
  The cluster will enter a **Hot Loop** where it starts and kills pods forever, destroying your performance. This usually happens when people use two different automation tools (like Helm and a custom script) to manage the same resource.

---

### Key Takeaways

- **Declarative** = You tell K8s what you want, not how to do it.
- **Reconciliation Loop** = The engine that makes reality match your desires.
- K8s is essentially a bunch of "Thermostats" working together.

### Common Mistakes

- Trying to "fix" pods manually (e.g., SSHing in to change a file). **The Reconciliation Loop will destroy your changes** within minutes because they don't match the Desired State in the YAML.

---

### üí° Knowledge Check

If you use `kubectl edit` to change the version of your app directly in the cluster, but your GitHub repo still has the old version in its YAML file, what might happen the next time your CI/CD pipeline runs?
