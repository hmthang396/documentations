# Phase 4: Storage & State

## 1. Why State is Hard in Kubernetes (The Ephemerality Problem)

Kubernetes was originally designed for **Stateless** apps (like web servers). Stateless apps are like **Cattle**: if one dies, you just replace it with an identical one, and nobody cares.

**Stateful apps** (Databases, Cache, Queues) are like **Pets**. If a Database dies, it _must_ come back with the same data it had before, or your business is in trouble.

### The Problem

By default, when a Pod is deleted, its internal filesystem is **wiped clean**. If your MySQL database saves data to `/var/lib/mysql` inside the container, and that pod is moved to another server, that data is **gone forever**.

---

## 2. PV and PVC: The Plug and the Socket

To solve this, Kubernetes separates the **Storage** from the **Pod**.

### The Mental Model: The Wall Outlet

**PersistentVolume** (hay **PV** trong Kubernetes) l√† m·ªôt **t√†i nguy√™n l∆∞u tr·ªØ b·ªÅn v·ªØng** (persistent storage resource) trong c·ª•m Kubernetes, ƒë·∫°i di·ªán cho m·ªôt **kh·ªëi l∆∞u tr·ªØ th·ª±c t·∫ø** (nh∆∞ ƒëƒ©a c·ª©ng, NFS share, cloud disk EBS/GCE PD/Azure Disk, Ceph RBD...) ƒë√£ ƒë∆∞·ª£c provision (cung c·∫•p) s·∫µn.

N√≥i ng·∫Øn g·ªçn:  
PV l√† **"k√©t s·∫Øt l∆∞u tr·ªØ"** c·∫•p c·ª•m (cluster-level), t·ªìn t·∫°i ƒë·ªôc l·∫≠p v·ªõi Pod. D·ªØ li·ªáu trong PV **kh√¥ng m·∫•t** khi Pod ch·∫øt, restart, reschedule, ho·∫∑c th·∫≠m ch√≠ x√≥a Deployment/StatefulSet (t√πy reclaim policy). N√≥ gi·ªëng nh∆∞ m·ªôt "·ªï c·ª©ng g·∫Øn ngo√†i" m√† Pod c√≥ th·ªÉ "m∆∞·ª£n" ƒë·ªÉ l∆∞u d·ªØ li·ªáu l√¢u d√†i.

**PersistentVolumeClaim** (hay **PVC** trong Kubernetes) l√† m·ªôt **t√†i nguy√™n y√™u c·∫ßu l∆∞u tr·ªØ** (storage request resource) m√† developer ho·∫∑c ·ª©ng d·ª•ng t·∫°o ra ƒë·ªÉ "y√™u c·∫ßu" m·ªôt ph·∫ßn l∆∞u tr·ªØ b·ªÅn v·ªØng t·ª´ c·ª•m. PVC gi·ªëng nh∆∞ m·ªôt **"phi·∫øu m∆∞·ª£n k√©t s·∫Øt"** ‚Äî b·∫°n ch·ªâ c·∫ßn khai b√°o nhu c·∫ßu (dung l∆∞·ª£ng, access mode, storage class...), Kubernetes s·∫Ω t·ª± ƒë·ªông t√¨m ho·∫∑c t·∫°o **PersistentVolume (PV)** ph√π h·ª£p ƒë·ªÉ bind (g·∫Øn k·∫øt) v·ªõi PVC ƒë√≥.

N√≥i ng·∫Øn g·ªçn:

- **PV** l√† "k√©t s·∫Øt th·ª±c t·∫ø" (do admin provision ho·∫∑c dynamic t·∫°o).
- **PVC** l√† "gi·∫•y y√™u c·∫ßu m∆∞·ª£n k√©t" (do developer t·∫°o).
- Khi PVC bind th√†nh c√¥ng v·ªõi PV ‚Üí Pod c√≥ th·ªÉ mount PVC ƒë·ªÉ d√πng l∆∞u tr·ªØ b·ªÅn v·ªØng, d·ªØ li·ªáu kh√¥ng m·∫•t khi Pod ch·∫øt/reschedule.

- **PersistentVolume (PV) - The Power Grid (Wall Outlet):** This is the actual physical disk (an AWS EBS volume, a GCP Disk, or a folder on a server). It's provided by the Administrator.
- **PersistentVolumeClaim (PVC) - The Power Cord (Plug):** This is the request from the Developer. "I need 10GB of storage with Read/Write access."

### Why do we need this?

It allows the Developer to say "I need a disk" without knowing _how_ that disk is made. They don't need to know if it's an SSD on Amazon or a hard drive in a local data center. They just "plug in" their claim.

---

## 3. StorageClass: The Vending Machine

In a large company, you don't want to wait for an administrator to manually create a PV every time a developer needs 1GB.

### The Mental Model: The Vending Machine

A **StorageClass** is a template. When a developer creates a PVC, the "StorageClass" automatically talks to the cloud provider, creates a disk, and wraps it in a PV. This is called **Dynamic Provisioning**.

---

## 4. StatefulSet: The "Pet" Controller

We learned about **Deployments** (for cattle) and **StatefulSets** (for pets).

### How StatefulSet handles storage:

When you use a StatefulSet, each pod gets its **own unique PVC**.

- `mysql-0` gets `data-mysql-0`
- `mysql-1` gets `data-mysql-1`

If `mysql-0` dies and is reborn on a different server, Kubernetes is smart enough to find the `data-mysql-0` disk and re-attach it to the new pod. **The identity and the data follow the pod.**

---

### What breaks in production? (Data Loss Scenarios)

1.  **The "Deletion" Trap:** By default, if you delete a PVC, the underlying PV (and all your data) might be deleted too (depending on the **Reclaim Policy**).
2.  **Multi-AZ Failures:** If your disk is in `US-East-1a` (Amazon Zone), but Kubernetes tries to start your Pod in `US-East-1b`, the Pod will stay in **Pending** forever. Disks usually cannot "travel" between zones.
3.  **ReadWriteOnce (RWO) Confusion:** Most cloud disks can only be attached to **one server at a time**. If you try to have 3 Pods on 3 different servers writing to the same disk, it will fail.

---

### Key Takeaways

- **PV** is the physical resource; **PVC** is the request for that resource.
- **StorageClass** automates the creation of disks.
- **StatefulSets** ensure that the same Pod always gets the same Disk.

### Common Mistakes

- Using `hostPath` (storing data on the server's local disk) in production. If the server dies, your data dies. Only use this for local testing.
- Thinking that "StatefulSet" magically backups your data. **It doesn't.** It only ensures the disk is re-attached. You still need a backup strategy!

### Production Tips

- Always set your **Reclaim Policy** to `Retain` for production databases. This way, if you accidentally delete the PVC, the physical disk is kept safe.
- Monitor your disk usage! A "Disk Full" error is the #1 cause of database corruption in Kubernetes.

---

### üí° Knowledge Check

If you have a cluster with 3 nodes, and you are using a standard AWS EBS volume (which is `ReadWriteOnce`), can you scale your WordPress "Uploads" folder to 3 pods across all 3 nodes? Why or why not?
